# Data engineer bootcamp projects


| Name | Topic | Task description | Tools |
| :---------------------- | :---------------------- | :---------------------- | :---------------------- |
| [01. RFM data mart](https://github.com) | Data Quality; Data Marts; SQL | Create Recency, Frequency, Monetary Value segmentation mart and check data quality | *`PostgreSQL`* |
| [02. Data model re-build](https://github.com) | Data Layers; Dimensions; Facts; Views | DWH: Re-build DB schema | *`Python`* *`PostgreSQL`* |
| [03. ETL update](https://github.com) | Airflow Connections; DAGs; ETL; Batch Processing | Change existing pipeline considering modifications in DB | *`Python`* *`S3`* *`PostgreSQL`* *`Rest-API`* *`Airflow`* |
| [04. No project](https://github.com) | Data quality ||  |
| [05. Multiple sources](https://github.com) | Data Layers; Data Marts; SCD; Incremental Loading | E-shop DWH from multiple sources with Apache Airflow | *`Python`* *`Docker`* *`PostgreSQL`* *`MongoDB`* *`Airflow`* |
| [06. Analytical DWH](https://github.com) | Analytical DBMS; Distributed Data Processing (DDP); Data Models; Data Vault | Build an analytical storage based on Vertica using Data Vault storage model | *`Python`* *`Rest-API`* *`S3`* *`Vertica`* *`Airflow`* |
| [07. Geospatial data](https://github.com) | Apache Spark; Data Lake; Big Data; Batch Processing; Geospatial Data | Create data marts on regular basis in Apache Hadoop file system | *`Apache Spark`* *`Hadoop`* *`HDFS`* |
| [08. Realtime data](https://github.com) | Stream Processing; Apache Spark Structured Streaming; Apache Kafka; Consumer; Producer; Stream-Static Join | Receive messages from Kafka, process and send to Postgres and new topic for the Kafka broker | *`Kafka`* *`PostgreSQL`* *`PySpark`* *`Spark Streaming`* *`Python`* |
| [09. Cloud micro services](https://github.com) | Stream Processing; Microservice Architecture; Cloud services | Receive real-time data from the Kafka broker, process and decompose into different layers of the data warehouse in cloud with Docker and k8s| *`Kafka`* *`PostgreSQL`* *`Redis`* *`Kubernetes`* *`Python`* *`Docker`* *`Redis`* *`Kubernetes`* *`Yandex cloud`*|
| [10. Multiple source DWH](https://github.com) | Batch processing; Data Mart; DAGs | ETL pipeline, which retrieves data from Postgres and uploads data extract to the Vertica DWH | *`Vertica`* *`PostgreSQL`* *`Airflow`* *`Metabase`* *`Python`* |
