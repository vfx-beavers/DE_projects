# Data engineer bootcamp projects


| Name | Topic | Task description | Tools |
| :---------------------- | :---------------------- | :---------------------- | :---------------------- |
| [01. Simple data mart](https://github.com) | Data Quality; Data Marts; SQL | Create RFM segmentation and check data quality | *`PostgreSQL`* |
| [02. Data model re-build](https://github.com) | Data Layers; Dimensions; Facts; Views | DWH: Re-build DB schema | *`Python`* *`PostgreSQL`* |
| [03. ETL update](https://github.com) | Airflow Connections; DAGs; ETL; Batch Processing | Change existing pipeline considering modifications in DB | *`Python`* *`S3`* *`PostgreSQL`* *`Rest-API`* *`Airflow`* |
| [04. Multiple sources](https://github.com) | Data Layers; Data Marts; SCD; Incremental Loading | E-shop DWH from multiple sources with Apache Airflow | *`Python`* *`Docker`* *`PostgreSQL`* *`MongoDB`* *`Airflow`* |
| [05. No project](https://github.com) |  ||  |
| [06. Analytical DWH](https://github.com) | Analytical DBMS; Distributed Data Processing (DDP);
Data Models; Data Vault | Build an analytical storage based on Vertica using Data Vault storage model | *`Python`* *`Rest-API`* *`S3`* *`Vertica`* *`Airflow`* |
| [07. Social network](https://github.com) | Apache Spark; Data Lake; Big Data; Batch Processing; Geospatial Data | Create data marts on regular basis in Apache Hadoop file system | *`Apache Spark`* *`Hadoop`* *`HDFS`* |
| [08. Restaurant promotions](https://github.com) | Real time data processing | Receive messages from Kafka, process and send to two receivers: a Postgres database and a new topic for the Kafka broker | *`PySpark`* *`Kafka`* *`PostgreSQL`* |
| [09. Micro services](https://github.com) | Cloud services | Receive real-time data from the Kafka broker, process and decompose into different layers of the data warehouse | *`Kafka`* *`PostgreSQL`* *`Redis`* *`Kubernetes`* *`Python`* |
| [09. Final](https://github.com) | Batch processing | Create a pipeline, which retrieves data from Postgres for the given time period and uploads data extract to the Vertica | *`Vertica`* *`PostgreSQL`* *`Airflow`* *`Metabase`* *`Python`* |
